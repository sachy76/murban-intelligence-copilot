# Murban Intelligence Copilot - Environment Configuration
# Copy this file to .env and customize for your environment

# =============================================================================
# LLM Configuration
# =============================================================================

# Path to LLM configuration file (optional - defaults to config/llm_config.yaml)
# MURBAN_LLM_CONFIG=/path/to/llm_config.yaml

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
MURBAN_LOG_LEVEL=INFO

# Log directory (default: ./logs)
# MURBAN_LOG_DIR=/var/log/murban-copilot

# =============================================================================
# Application Settings
# =============================================================================

# Cache directory for LLM responses (default: .llm_cache)
# MURBAN_CACHE_DIR=/tmp/murban-cache

# =============================================================================
# Streamlit Configuration
# =============================================================================

# Server port (default: 8501)
STREAMLIT_SERVER_PORT=8501

# Server address (default: localhost, use 0.0.0.0 for Docker)
STREAMLIT_SERVER_ADDRESS=localhost

# Run in headless mode (set to true for Docker/server deployments)
STREAMLIT_SERVER_HEADLESS=false

# Disable usage stats collection
STREAMLIT_BROWSER_GATHER_USAGE_STATS=false

# =============================================================================
# Market Data Configuration
# =============================================================================

# Request timeout in seconds (default: 60)
# MURBAN_MARKET_DATA_TIMEOUT=60

# Maximum retry attempts for transient failures (default: 3)
# MURBAN_MARKET_DATA_MAX_RETRIES=3

# =============================================================================
# HuggingFace Configuration (for LLM model downloads)
# =============================================================================

# HuggingFace token for private models (optional)
# HF_TOKEN=your_huggingface_token

# HuggingFace cache directory (optional)
# HF_HOME=/path/to/huggingface/cache

# =============================================================================
# Development Settings
# =============================================================================

# Enable debug mode (default: false)
# MURBAN_DEBUG=false

# Use mock LLM client for testing (default: false)
# MURBAN_USE_MOCK_LLM=false
