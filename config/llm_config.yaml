# =============================================================================
# Murban Intelligence Copilot - Application Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
# Model type options:
#   - "llama": Use llama-cpp-python for GGUF models (local inference)
#   - "transformers": Use HuggingFace transformers (supports classification models)
#
# To switch models, change model_type and update model_repo accordingly.
# -----------------------------------------------------------------------------
llm:
  defaults:
    n_ctx: 4096
    n_gpu_layers: -1
    cache_enabled: true
    verbose: false
    # Default inference parameters (used when not specified per-model)
    inference:
      max_tokens: 512
      temperature: 0.7
    # Analysis-specific inference defaults
    analysis_inference:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      frequency_penalty: 0.3
      presence_penalty: 0.1
    # Extraction-specific inference defaults
    extraction_inference:
      max_tokens: 150
      temperature: 0.1
      top_p: 0.65
      top_k: 25
      frequency_penalty: 0.0
      presence_penalty: 0.0

  # -------------------------------------------------------------------------
  # Analysis Model Configuration
  # -------------------------------------------------------------------------
  # This model generates comprehensive market analysis text.
  # Recommended: Use a generative LLM (llama type) for rich analysis.
  analysis:
    model_type: "llama"  # Options: "llama" | "transformers"
    # --- Llama/GGUF Model (default) ---
    model_repo: "MaziyarPanahi/gemma-3-12b-it-GGUF"
    model_file: "gemma-3-12b-it.Q6_K.gguf"
    # --- Alternative: Smaller Llama model ---
    # model_repo: "bartowski/gemma-2-9b-it-GGUF"
    # model_file: "gemma-2-9b-it-Q4_K_M.gguf"
    inference:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      frequency_penalty: 0.3
      presence_penalty: 0.1

  # -------------------------------------------------------------------------
  # Extraction Model Configuration
  # -------------------------------------------------------------------------
  # This model extracts structured signals from the analysis text.
  # Options:
  #   1. Llama (default): Uses prompted extraction with a generative model
  #   2. Transformers: Uses a fine-tuned sentiment classification model
  # -------------------------------------------------------------------------
  extraction:
    #model_type: "llama"  # Options: "llama" | "transformers"
    # --- Llama/GGUF Model (default) ---
    #model_repo: "bartowski/gemma-2-9b-it-GGUF"
    #model_file: "gemma-2-9b-it-Q4_K_M.gguf"
    # --- Alternative: Financial Sentiment Model (transformers) ---
    model_type: "transformers"
    model_repo: "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
    # task: "sentiment-analysis"
    # device: "auto"  # Options: "auto" | "cpu" | "cuda" | "mps"
    # --- Alternative: ProsusAI FinBERT ---
    #model_type: "transformers"
    #model_repo: "ProsusAI/finbert"
    # task: "sentiment-analysis"
    # device: "auto"
    inference:
      max_tokens: 150   # Forces brevity
      temperature: 0.1  # Prevents interpretive drift
      top_p: 0.65       # Tight probability mass
      top_k: 25         # Constrained vocabulary
      frequency_penalty: 0.0  # No repetition risk
      presence_penalty: 0.0   # Prevents new ideas
    # Transformers-specific settings (only used when model_type is "transformers")
    task: "sentiment-analysis"
    device: "cpu"  # Use CPU to avoid MPS memory issues

# -----------------------------------------------------------------------------
# Cache Configuration
# -----------------------------------------------------------------------------
cache:
  directory: ".llm_cache"
  enabled: true

# -----------------------------------------------------------------------------
# Market Data Configuration
# -----------------------------------------------------------------------------
market_data:
  # Ticker symbols (Yahoo Finance format)
  wti_ticker: "CL=F"
  brent_ticker: "BZ=F"

  # Request settings
  timeout: 60
  max_retries: 3
  min_retry_wait: 1
  max_retry_wait: 10

  # Data fetching
  buffer_days: 10
  latest_price_lookback_days: 7

# -----------------------------------------------------------------------------
# Analysis Configuration
# -----------------------------------------------------------------------------
analysis:
  # Moving average windows (in days)
  short_ma_window: 5
  long_ma_window: 20

  # Outlier detection threshold (MAD units)
  outlier_threshold: 3.0

  # Data processing
  gap_fill_threshold: 2
  min_data_points: 5

# -----------------------------------------------------------------------------
# Signal Generation Configuration
# -----------------------------------------------------------------------------
signal:
  # Default values when extraction fails
  default_signal: "neutral"
  default_confidence: 0.5

  # Keywords for fallback classification
  bullish_keywords:
    - "bullish"
    - "upward"
    - "positive"
    - "buy"
  bearish_keywords:
    - "bearish"
    - "downward"
    - "negative"
    - "sell"

# -----------------------------------------------------------------------------
# UI Configuration
# -----------------------------------------------------------------------------
ui:
  # Historical days slider bounds
  min_historical_days: 7
  max_historical_days: 90
  default_historical_days: 30

  # Sample data for demo mode (when market data unavailable)
  sample_wti_base_price: 85.0
  sample_brent_base_price: 82.0
  sample_price_variation: 0.5
