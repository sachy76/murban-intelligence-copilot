llm:
  defaults:
    n_ctx: 4096
    n_gpu_layers: -1
    cache_enabled: true
    verbose: false

  analysis:
    model_repo: "MaziyarPanahi/gemma-3-12b-it-GGUF"
    model_file: "gemma-3-12b-it.Q6_K.gguf"
    inference:
      max_tokens: 2048
      temperature: 0.7

  extraction:
    model_repo: "bartowski/gemma-2-9b-it-GGUF"
    model_file: "gemma-2-9b-it-Q4_K_M.gguf"
    inference:
      max_tokens: 1024
      temperature: 0.3

cache:
  directory: ".llm_cache"
  enabled: true
